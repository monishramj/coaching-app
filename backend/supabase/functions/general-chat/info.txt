// 1. user sends message
history.push({
  role: "user",
  parts: [{ text: userInput }]
})

// 2. send request with full history
const response = await model.generateContent({ history })

// 3. append model response
history.push({
  role: "model",
  parts: [{ text: response.text() }]
})

There is a way for me to deal with user histoy for the past n messages


Local Storage
Idea: I store 35 messages worth of conversation at once. (70 elements in the array of 1 role:user and 1 role:model)
Whent this limit is reached, I wipe 25 messages (all even indices) and send to summar function to go to long term mem
Then i do .trim or whatever to cut history down to 10 (or 20 elements)

This way model retains last 10 memories woth when generation automatically and vector embedding decides
which memories are important
-------------------------------

Notes about vector encoding: 
Keep summaries 2–3 sentences per bullet, 2–5 bullets per 50-message chunk.
Include user and assistant content.
Store each bullet as a separate row, or keep all bullets from the chunk in a single row — either works, just be consistent.
When retrieving, fetch multiple rows to give the model enough context.